

```python
import random
import matplotlib.pyplot as plt

from keras import models
from keras import layers
from keras import optimizers
from keras import regularizers
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
```


```python
random.seed(1234)
#load data
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

#transform image data into 2d-tensor
x_train = x_train.reshape((60000, 28*28))
x_train = x_train.astype('float32') / 255

x_test = x_test.reshape((10000, 28*28))
x_test = x_test.astype('float32')/ 255

#make y categorical type
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

#check shapes
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)
```

    (60000, 784) (60000, 10) (10000, 784) (10000, 10)
    


```python
random.seed(1234)
# Model 4 layer 5, units 128, l2 reg weight 0.001
model_4 = models.Sequential()
model_4.add(layers.Dense(128, activation = 'relu', 
                         input_shape=(784,),
                         kernel_regularizer = regularizers.l2(0.001)))

model_4.add(layers.Dense(128, activation = 'relu', 
                         kernel_regularizer = regularizers.l2(0.001)))

model_4.add(layers.Dense(128, activation = 'relu', 
                         kernel_regularizer = regularizers.l2(0.001)))

model_4.add(layers.Dense(128, activation = 'relu', 
                         kernel_regularizer = regularizers.l2(0.001)))

model_4.add(layers.Dense(128, activation = 'relu', 
                         kernel_regularizer = regularizers.l2(0.001)))

model_4.add(layers.Dense(10, activation = 'softmax'))

model_4.compile(optimizer = 'rmsprop',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

model_4.fit(x_train, y_train, 
                    batch_size = 512, epochs = 200, verbose = 0)
```




    <keras.callbacks.History at 0x7fbcb0e07f98>




```python
model_4.evaluate(x_test, y_test)
```

    10000/10000 [==============================] - 1s 66us/step
    




    [0.4529171320438385, 0.8777]



* The loss and accuracy look pretty decent; the validation accuracy is about 87.8%. the model I think achieve some degree of generalizability.

* The followings are some thoughts after experimenting with neural nets.

    - I realized that using a simple model might not produce the best result; for example, I learned that using weight regularization can improve the model fit when it is used appropriately (which should be neither too high or low). Of course, the proper way of implementing sorts of additional settings would depend on the data.
    
    - The optimal sizes of layers and hidden units also seem to depend on the data; I first thought that increasing the number of layers and hidden units would lead to a better model fit but it was not the case for the data I worked with for this assignment. For instance, when I constructed a model which has 20 layers with 128 hidden units and 0.001 L2 regularization weight, the model stopped minimizing the loss function. I guess the optimization process was stuck to a local minimum and it might have been different with another set of data.
    
    - I think that similar point could be pointed out regarding the epoch size; a larger epoch size does not seem to guarantee a better result; the model can degrade as the number of epoch increases in some cases.
    
    - In general, I think I learned why people say that we need a sense of art, rather than science when working with the neural nets.
