{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW01\n",
    "<br>\n",
    "<br>\n",
    "Di Tong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras import models, layers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Set your random seed to 1234\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ditong/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the Fashion-MNIST dataset\n",
    "\n",
    "# Preprocess the data by converting the data to a 2D tensor with individual values between 0 and 1\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Randomly split the training data into 50,000 training observations and 10,000 validation observations\n",
    "train_x, val_x, train_y, val_y = train_test_split(x_train, y_train, train_size = 50000, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ditong/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ditong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.8699 - acc: 0.6880 - val_loss: 0.6127 - val_acc: 0.7598\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.5155 - acc: 0.8077 - val_loss: 0.5211 - val_acc: 0.7997\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.4351 - acc: 0.8373 - val_loss: 0.4167 - val_acc: 0.8525\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.3895 - acc: 0.8551 - val_loss: 0.4207 - val_acc: 0.8508\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.3548 - acc: 0.8672 - val_loss: 0.3508 - val_acc: 0.8738\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.3328 - acc: 0.8756 - val_loss: 0.3613 - val_acc: 0.8644\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.3172 - acc: 0.8809 - val_loss: 0.3890 - val_acc: 0.8549\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.2951 - acc: 0.8882 - val_loss: 0.3432 - val_acc: 0.8728\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.2836 - acc: 0.8921 - val_loss: 0.3710 - val_acc: 0.8666\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.2687 - acc: 0.8976 - val_loss: 0.3749 - val_acc: 0.8721\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.2575 - acc: 0.9020 - val_loss: 0.3431 - val_acc: 0.8852\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.2453 - acc: 0.9049 - val_loss: 0.3658 - val_acc: 0.8710\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.2377 - acc: 0.9095 - val_loss: 0.3184 - val_acc: 0.8923\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.2270 - acc: 0.9126 - val_loss: 0.3984 - val_acc: 0.8727\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.2227 - acc: 0.9150 - val_loss: 0.3798 - val_acc: 0.8793\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.2152 - acc: 0.9166 - val_loss: 0.4415 - val_acc: 0.8626\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.2087 - acc: 0.9193 - val_loss: 0.3922 - val_acc: 0.8858\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.2010 - acc: 0.9228 - val_loss: 0.3855 - val_acc: 0.8874\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.1960 - acc: 0.9235 - val_loss: 0.4194 - val_acc: 0.8870\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1886 - acc: 0.9271 - val_loss: 0.3745 - val_acc: 0.8846\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.1867 - acc: 0.9285 - val_loss: 0.3885 - val_acc: 0.8788\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1797 - acc: 0.9297 - val_loss: 0.3790 - val_acc: 0.8772\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.1784 - acc: 0.9314 - val_loss: 0.4347 - val_acc: 0.8707\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.1732 - acc: 0.9340 - val_loss: 0.5190 - val_acc: 0.8669\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.1730 - acc: 0.9354 - val_loss: 0.3724 - val_acc: 0.8947\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.1666 - acc: 0.9373 - val_loss: 0.4255 - val_acc: 0.8795\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.1557 - acc: 0.9395 - val_loss: 0.4180 - val_acc: 0.8975\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.1597 - acc: 0.9384 - val_loss: 0.4079 - val_acc: 0.8853\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.1549 - acc: 0.9423 - val_loss: 0.4571 - val_acc: 0.8828\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1558 - acc: 0.9418 - val_loss: 0.4571 - val_acc: 0.8931\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1518 - acc: 0.9431 - val_loss: 0.4220 - val_acc: 0.8970\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1492 - acc: 0.9441 - val_loss: 0.4548 - val_acc: 0.8890\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1509 - acc: 0.9441 - val_loss: 0.4035 - val_acc: 0.8838\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1420 - acc: 0.9463 - val_loss: 0.4883 - val_acc: 0.8867\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1408 - acc: 0.9488 - val_loss: 0.4780 - val_acc: 0.8852\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1419 - acc: 0.9470 - val_loss: 0.4712 - val_acc: 0.8781\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1358 - acc: 0.9498 - val_loss: 0.4574 - val_acc: 0.8900\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1385 - acc: 0.9496 - val_loss: 0.4716 - val_acc: 0.8946\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1333 - acc: 0.9497 - val_loss: 0.4162 - val_acc: 0.8980\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.1295 - acc: 0.9515 - val_loss: 0.5051 - val_acc: 0.8819\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1318 - acc: 0.9516 - val_loss: 0.6494 - val_acc: 0.8767\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1253 - acc: 0.9523 - val_loss: 0.4817 - val_acc: 0.8935\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1269 - acc: 0.9534 - val_loss: 0.5517 - val_acc: 0.8903\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1242 - acc: 0.9549 - val_loss: 0.4962 - val_acc: 0.8865\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1265 - acc: 0.9540 - val_loss: 0.4882 - val_acc: 0.8944\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1234 - acc: 0.9555 - val_loss: 0.4895 - val_acc: 0.8863\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1198 - acc: 0.9574 - val_loss: 0.4423 - val_acc: 0.8950\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.1191 - acc: 0.9570 - val_loss: 0.4772 - val_acc: 0.9017\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.1179 - acc: 0.9568 - val_loss: 0.5178 - val_acc: 0.8973\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1221 - acc: 0.9580 - val_loss: 0.5344 - val_acc: 0.8956\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1203 - acc: 0.9578 - val_loss: 0.5234 - val_acc: 0.8983\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1161 - acc: 0.9579 - val_loss: 0.5649 - val_acc: 0.8885\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1194 - acc: 0.9588 - val_loss: 0.5781 - val_acc: 0.8906\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1155 - acc: 0.9598 - val_loss: 0.5935 - val_acc: 0.8727\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1179 - acc: 0.9581 - val_loss: 0.9127 - val_acc: 0.8648\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1155 - acc: 0.9584 - val_loss: 0.5145 - val_acc: 0.8903\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1102 - acc: 0.9604 - val_loss: 0.5037 - val_acc: 0.8964\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1031 - acc: 0.9613 - val_loss: 0.7423 - val_acc: 0.8727\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1173 - acc: 0.9601 - val_loss: 0.5479 - val_acc: 0.8981\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1104 - acc: 0.9622 - val_loss: 0.6509 - val_acc: 0.8866\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1068 - acc: 0.9623 - val_loss: 0.5624 - val_acc: 0.8997\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1093 - acc: 0.9624 - val_loss: 0.4891 - val_acc: 0.8954\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1109 - acc: 0.9618 - val_loss: 0.5647 - val_acc: 0.8925\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0970 - acc: 0.9659 - val_loss: 0.5600 - val_acc: 0.8942\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1157 - acc: 0.9628 - val_loss: 0.5812 - val_acc: 0.9009\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1077 - acc: 0.9655 - val_loss: 0.8700 - val_acc: 0.8668\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1091 - acc: 0.9646 - val_loss: 0.5374 - val_acc: 0.9000\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1023 - acc: 0.9651 - val_loss: 0.5572 - val_acc: 0.8947\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1043 - acc: 0.9652 - val_loss: 0.5630 - val_acc: 0.8938\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1040 - acc: 0.9651 - val_loss: 0.5699 - val_acc: 0.8959\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0964 - acc: 0.9662 - val_loss: 0.5744 - val_acc: 0.9008\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1003 - acc: 0.9671 - val_loss: 0.7878 - val_acc: 0.8764\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1005 - acc: 0.9650 - val_loss: 0.6054 - val_acc: 0.9003\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.1081 - acc: 0.9654 - val_loss: 0.5351 - val_acc: 0.8999\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0893 - acc: 0.9680 - val_loss: 0.6618 - val_acc: 0.8981\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1004 - acc: 0.9665 - val_loss: 0.6404 - val_acc: 0.8874\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0983 - acc: 0.9676 - val_loss: 0.5929 - val_acc: 0.9019\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1005 - acc: 0.9658 - val_loss: 0.5405 - val_acc: 0.8998\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0986 - acc: 0.9687 - val_loss: 0.5721 - val_acc: 0.8973\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0953 - acc: 0.9690 - val_loss: 0.5897 - val_acc: 0.8983\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0937 - acc: 0.9695 - val_loss: 0.6353 - val_acc: 0.8989\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0959 - acc: 0.9681 - val_loss: 0.6753 - val_acc: 0.8968\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.0949 - acc: 0.9699 - val_loss: 0.6354 - val_acc: 0.8961\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0908 - acc: 0.9684 - val_loss: 0.5659 - val_acc: 0.8992\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0902 - acc: 0.9707 - val_loss: 0.6622 - val_acc: 0.8935\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0895 - acc: 0.9699 - val_loss: 0.6013 - val_acc: 0.8954\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0936 - acc: 0.9702 - val_loss: 0.6532 - val_acc: 0.9018\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0886 - acc: 0.9708 - val_loss: 0.6259 - val_acc: 0.8980\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0923 - acc: 0.9718 - val_loss: 0.6883 - val_acc: 0.8834\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.0919 - acc: 0.9702 - val_loss: 0.6542 - val_acc: 0.8970\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.0847 - acc: 0.9725 - val_loss: 0.6350 - val_acc: 0.8956\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0833 - acc: 0.9721 - val_loss: 0.6998 - val_acc: 0.8958\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.6939 - val_acc: 0.8915\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0933 - acc: 0.9704 - val_loss: 0.6707 - val_acc: 0.8937\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.0880 - acc: 0.9736 - val_loss: 0.6728 - val_acc: 0.8941\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0961 - acc: 0.9710 - val_loss: 0.6697 - val_acc: 0.8970\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0858 - acc: 0.9734 - val_loss: 0.6012 - val_acc: 0.8862\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0905 - acc: 0.9737 - val_loss: 0.8580 - val_acc: 0.8943\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0914 - acc: 0.9721 - val_loss: 0.7012 - val_acc: 0.8854\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0800 - acc: 0.9738 - val_loss: 0.6345 - val_acc: 0.8987\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.0837 - acc: 0.9738 - val_loss: 0.6912 - val_acc: 0.8898\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0829 - acc: 0.9741 - val_loss: 0.6364 - val_acc: 0.8992\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0784 - acc: 0.9755 - val_loss: 0.7857 - val_acc: 0.8933\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.0899 - acc: 0.9728 - val_loss: 0.6505 - val_acc: 0.9005\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0823 - acc: 0.9750 - val_loss: 0.7120 - val_acc: 0.8893\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0860 - acc: 0.9744 - val_loss: 0.8216 - val_acc: 0.8876\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0774 - acc: 0.9754 - val_loss: 0.7419 - val_acc: 0.8907\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.0844 - acc: 0.9760 - val_loss: 0.6671 - val_acc: 0.9005\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.0788 - acc: 0.9756 - val_loss: 0.7507 - val_acc: 0.8918\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.0724 - acc: 0.9766 - val_loss: 0.7146 - val_acc: 0.8956\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0789 - acc: 0.9761 - val_loss: 0.6024 - val_acc: 0.8983\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0818 - acc: 0.9759 - val_loss: 0.7436 - val_acc: 0.8928\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.0823 - acc: 0.9756 - val_loss: 0.6776 - val_acc: 0.8995\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0765 - acc: 0.9768 - val_loss: 0.7896 - val_acc: 0.8969\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0857 - acc: 0.9760 - val_loss: 0.6412 - val_acc: 0.9009\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.0835 - acc: 0.9777 - val_loss: 0.7968 - val_acc: 0.8825\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0748 - acc: 0.9767 - val_loss: 0.7522 - val_acc: 0.8955\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0772 - acc: 0.9771 - val_loss: 0.6486 - val_acc: 0.8962\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0701 - acc: 0.9786 - val_loss: 0.7938 - val_acc: 0.8922\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.0734 - acc: 0.9787 - val_loss: 0.8445 - val_acc: 0.8880\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0785 - acc: 0.9770 - val_loss: 0.7036 - val_acc: 0.8978\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0783 - acc: 0.9777 - val_loss: 0.9210 - val_acc: 0.8922\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0739 - acc: 0.9785 - val_loss: 0.6791 - val_acc: 0.8974\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0661 - acc: 0.9801 - val_loss: 0.8492 - val_acc: 0.8949\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.0716 - acc: 0.9783 - val_loss: 0.7501 - val_acc: 0.8897\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0768 - acc: 0.9781 - val_loss: 0.6926 - val_acc: 0.8950\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0838 - acc: 0.9773 - val_loss: 0.6622 - val_acc: 0.9003\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0725 - acc: 0.9804 - val_loss: 0.7614 - val_acc: 0.8986\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.0789 - acc: 0.9779 - val_loss: 0.6876 - val_acc: 0.9024\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0698 - acc: 0.9803 - val_loss: 0.6711 - val_acc: 0.9014\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0726 - acc: 0.9791 - val_loss: 0.6751 - val_acc: 0.8981\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0759 - acc: 0.9795 - val_loss: 0.7646 - val_acc: 0.8971\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0724 - acc: 0.9797 - val_loss: 0.7653 - val_acc: 0.8992\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0656 - acc: 0.9809 - val_loss: 0.8789 - val_acc: 0.8788\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0666 - acc: 0.9790 - val_loss: 0.6755 - val_acc: 0.8991\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0633 - acc: 0.9806 - val_loss: 0.7524 - val_acc: 0.8965\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0740 - acc: 0.9794 - val_loss: 0.8051 - val_acc: 0.8929\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0795 - acc: 0.9810 - val_loss: 0.8389 - val_acc: 0.8934\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0739 - acc: 0.9804 - val_loss: 0.8083 - val_acc: 0.8991\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0693 - acc: 0.9800 - val_loss: 0.7758 - val_acc: 0.8951\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.0805 - acc: 0.9799 - val_loss: 0.7669 - val_acc: 0.8822\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.0658 - acc: 0.9813 - val_loss: 0.6672 - val_acc: 0.9000\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0665 - acc: 0.9811 - val_loss: 0.7302 - val_acc: 0.9004\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0729 - acc: 0.9809 - val_loss: 0.7991 - val_acc: 0.8946\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.0726 - acc: 0.9802 - val_loss: 0.7566 - val_acc: 0.8922\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.0701 - acc: 0.9809 - val_loss: 1.1739 - val_acc: 0.8740\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0867 - acc: 0.9801 - val_loss: 0.7292 - val_acc: 0.9005\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0732 - acc: 0.9817 - val_loss: 0.7239 - val_acc: 0.9002\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0582 - acc: 0.9830 - val_loss: 0.8523 - val_acc: 0.8932\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0708 - acc: 0.9814 - val_loss: 0.7592 - val_acc: 0.8981\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0728 - acc: 0.9807 - val_loss: 0.8816 - val_acc: 0.8952\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0749 - acc: 0.9808 - val_loss: 0.8520 - val_acc: 0.8936\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0729 - acc: 0.9820 - val_loss: 0.8301 - val_acc: 0.8999\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0592 - acc: 0.9838 - val_loss: 0.7331 - val_acc: 0.8948\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0741 - acc: 0.9818 - val_loss: 0.8338 - val_acc: 0.8877\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0746 - acc: 0.9814 - val_loss: 0.7834 - val_acc: 0.8889\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0599 - acc: 0.9835 - val_loss: 0.8460 - val_acc: 0.9011\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0750 - acc: 0.9803 - val_loss: 0.7720 - val_acc: 0.8979\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0671 - acc: 0.9837 - val_loss: 0.9160 - val_acc: 0.8856\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0700 - acc: 0.9823 - val_loss: 0.7809 - val_acc: 0.8992\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0696 - acc: 0.9826 - val_loss: 0.7869 - val_acc: 0.9006\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0784 - acc: 0.9816 - val_loss: 0.8571 - val_acc: 0.8920\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.0735 - acc: 0.9824 - val_loss: 0.7543 - val_acc: 0.9004\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0651 - acc: 0.9832 - val_loss: 0.9301 - val_acc: 0.8887\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0628 - acc: 0.9832 - val_loss: 0.7540 - val_acc: 0.8984\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0765 - acc: 0.9819 - val_loss: 0.8089 - val_acc: 0.8885\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0613 - acc: 0.9839 - val_loss: 0.8174 - val_acc: 0.8993\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0630 - acc: 0.9846 - val_loss: 0.8977 - val_acc: 0.8906\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0692 - acc: 0.9832 - val_loss: 0.7903 - val_acc: 0.9034\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0652 - acc: 0.9838 - val_loss: 1.0418 - val_acc: 0.8710\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0746 - acc: 0.9823 - val_loss: 0.8446 - val_acc: 0.9021\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0874 - acc: 0.9807 - val_loss: 0.8238 - val_acc: 0.8891\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0677 - acc: 0.9832 - val_loss: 0.7519 - val_acc: 0.8972\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0674 - acc: 0.9843 - val_loss: 0.7958 - val_acc: 0.9002\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0612 - acc: 0.9843 - val_loss: 0.8521 - val_acc: 0.8829\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0702 - acc: 0.9826 - val_loss: 0.8306 - val_acc: 0.8963\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0627 - acc: 0.9842 - val_loss: 0.8130 - val_acc: 0.8914\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0662 - acc: 0.9835 - val_loss: 0.8920 - val_acc: 0.8986\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.0663 - acc: 0.9831 - val_loss: 0.8139 - val_acc: 0.8921\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0603 - acc: 0.9848 - val_loss: 0.7186 - val_acc: 0.8993\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.0576 - acc: 0.9850 - val_loss: 0.8388 - val_acc: 0.9007\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.0587 - acc: 0.9849 - val_loss: 0.8301 - val_acc: 0.8924\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0623 - acc: 0.9836 - val_loss: 0.8402 - val_acc: 0.8986\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0572 - acc: 0.9849 - val_loss: 1.0749 - val_acc: 0.8876\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0551 - acc: 0.9856 - val_loss: 0.8746 - val_acc: 0.8884\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0625 - acc: 0.9851 - val_loss: 0.8240 - val_acc: 0.9002\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.0579 - acc: 0.9851 - val_loss: 0.8163 - val_acc: 0.9015\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0629 - acc: 0.9846 - val_loss: 0.8095 - val_acc: 0.9020\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0720 - acc: 0.9828 - val_loss: 0.7449 - val_acc: 0.9024\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0603 - acc: 0.9858 - val_loss: 0.8633 - val_acc: 0.8988\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0681 - acc: 0.9852 - val_loss: 0.8728 - val_acc: 0.8952\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0752 - acc: 0.9829 - val_loss: 0.8400 - val_acc: 0.8876\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0609 - acc: 0.9851 - val_loss: 0.8343 - val_acc: 0.8954\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0571 - acc: 0.9861 - val_loss: 0.8495 - val_acc: 0.8930\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0572 - acc: 0.9860 - val_loss: 0.8631 - val_acc: 0.8968\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0539 - acc: 0.9872 - val_loss: 0.8479 - val_acc: 0.8972\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0610 - acc: 0.9849 - val_loss: 1.1125 - val_acc: 0.8873\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.0759 - acc: 0.9837 - val_loss: 0.8084 - val_acc: 0.9018\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.0600 - acc: 0.9863 - val_loss: 0.8341 - val_acc: 0.9020\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0696 - acc: 0.9839 - val_loss: 0.7547 - val_acc: 0.8987\n"
     ]
    }
   ],
   "source": [
    "# 3. Implement a series of neural network models\n",
    "\n",
    "#i. Initial test\n",
    "network_init = models.Sequential()\n",
    "network_init.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network_init.add(layers.Dense(512, activation='relu'))\n",
    "network_init.add(layers.Dense(512, activation='relu'))\n",
    "network_init.add(layers.Dense(512, activation='relu'))\n",
    "network_init.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network_init.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "init_result = network_init.fit(train_x, train_y, epochs=200, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation set accuracy and loss over the epochs\n",
    "init_val_acc = init_result.history['val_acc']\n",
    "init_val_loss = init_result.history['val_loss']\n",
    "epochs = np.arange(1, 201)\n",
    "\n",
    "plt.plot(epochs, init_val_acc, label='validation accuracy')\n",
    "plt.plot(epochs, init_val_loss, label='validation loss')\n",
    "plt.ylabel('Validation set accuracy & loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('The initial model validation accuracy and loss rates for different epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the epoch where the model's performance degrades based on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. implement dropout\n",
    "dropout = models.Sequential()\n",
    "dropout.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "dropout.add(layers.Dropout(0.5))\n",
    "dropout.add(layers.Dense(512, activation='relu'))\n",
    "dropout.add(layers.Dropout(0.5))\n",
    "dropout.add(layers.Dense(512, activation='relu'))\n",
    "dropout.add(layers.Dropout(0.5))\n",
    "dropout.add(layers.Dense(512, activation='relu'))\n",
    "dropout.add(layers.Dropout(0.5))\n",
    "dropout.add(layers.Dense(10, activation='softmax'))\n",
    "dropout.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_dropout = dropout.fit(train_x, train_y, epochs=200, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphically compare the validation loss across epochs to the initial model\n",
    "drop_val_loss = result_dropout.history['val_loss']\n",
    "plt.plot(epochs, init_val_loss, label='initial validation loss')\n",
    "plt.plot(epochs, drop_val_loss, label='dropout validation loss')\n",
    "plt.ylabel('Validation set loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.title('The initial and dropout model validation loss for different epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this new model perform relative to the old model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii. Weight regularization\n",
    "\n",
    "# Reestimate the initial model with L1 weight regularization on each layer \n",
    "l1_wr = models.Sequential()\n",
    "l1_wr.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "l1_wr.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "l1_wr.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "l1_wr.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "l1_wr.add(layers.Dense(10, activation='softmax'))\n",
    "l1_wr.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_l1 = l1_wr.fit(train_x, train_y, epochs=200, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reestimate the initial model with L2 weight regularization on each layer \n",
    "l2_wr = models.Sequential()\n",
    "l2_wr.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l2(0.001)))\n",
    "l2_wr.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "l2_wr.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "l2_wr.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "l2_wr.add(layers.Dense(10, activation='softmax'))\n",
    "l2_wr.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_l2 = l2_wr.fit(train_x, train_y, epochs=200, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss for the initial model vs. the dropout\n",
    "# vs. the L1 regularized model vs. the L2 regularized model\n",
    "l1_val_loss = result_l1.history['val_loss']\n",
    "l2_val_loss = result_l2.history['val_loss']\n",
    "plt.plot(epochs, init_val_loss, label='initial validation loss')\n",
    "plt.plot(epochs, drop_val_loss, label='dropout validation loss')\n",
    "plt.plot(epochs, l1_val_loss, label='L1 validation loss')\n",
    "plt.plot(epochs, l2_val_loss, label='L2 validation loss')\n",
    "plt.ylabel('Validation set loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.title('The initial, dropout, L1 and L2 regularized model validation loss for different epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which model appears to perform the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv. Evaluate a minimum of 10 alternative models, using different hyperparameter options \n",
    "\n",
    "# Model 1: 4 layer, 512 units, 512 batch size, 50 epochs, 0.01 l1 regularized\n",
    "m1 = models.Sequential()\n",
    "m1.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.01)))\n",
    "m1.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
    "m1.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
    "m1.add(layers.Dense(10, activation='softmax'))\n",
    "m1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m1 = m1.fit(train_x, train_y, epochs=50, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: 3 layer, 512 units, 512 batch size, 100 epochs, 0.01 l2 regularized\n",
    "m2 = models.Sequential()\n",
    "m2.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l2(0.01)))\n",
    "m2.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "m2.add(layers.Dense(10, activation='softmax'))\n",
    "m2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m1 = m2.fit(train_x, train_y, epochs=100, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: 5 layer, 256 units, 256 batch size, 50 epochs, 0.1 dropout rate\n",
    "m3 = models.Sequential()\n",
    "m3.add(layers.Dense(256, activation='relu', input_shape=(28 * 28,)))\n",
    "m3.add(layers.Dropout(0.1))\n",
    "m3.add(layers.Dense(256, activation='relu'))\n",
    "m3.add(layers.Dropout(0.1))\n",
    "m3.add(layers.Dense(256, activation='relu'))\n",
    "m3.add(layers.Dropout(0.1))\n",
    "m3.add(layers.Dense(256, activation='relu'))\n",
    "m3.add(layers.Dropout(0.1))\n",
    "m3.add(layers.Dense(10, activation='softmax'))\n",
    "m3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m3 = m3.fit(train_x, train_y, epochs=50, batch_size=256, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: 6 layer, 256 units, 256 batch size, 100 epochs, 0.1 dropout rate\n",
    "m4 = models.Sequential()\n",
    "m4.add(layers.Dense(256, activation='relu', input_shape=(28 * 28,)))\n",
    "m4.add(layers.Dropout(0.1))\n",
    "m4.add(layers.Dense(256, activation='relu'))\n",
    "m4.add(layers.Dropout(0.1))\n",
    "m4.add(layers.Dense(256, activation='relu'))\n",
    "m4.add(layers.Dropout(0.1))\n",
    "m4.add(layers.Dense(256, activation='relu'))\n",
    "m4.add(layers.Dropout(0.1))\n",
    "m4.add(layers.Dense(256, activation='relu'))\n",
    "m4.add(layers.Dropout(0.1))\n",
    "m4.add(layers.Dense(10, activation='softmax'))\n",
    "m4.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m4 = m4.fit(train_x, train_y, epochs=100, batch_size=256, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: 5 layer, 256 units, 256 batch size, 100 epochs, 0.05 l1 regularized\n",
    "m5 = models.Sequential()\n",
    "m5.add(layers.Dense(256, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.05)))\n",
    "m5.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.05)))\n",
    "m5.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.05)))\n",
    "m5.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.05)))\n",
    "m5.add(layers.Dense(10, activation='softmax'))\n",
    "m5.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m5 = m5.fit(train_x, train_y, epochs=100, batch_size=256, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6: 4 layer, 128 units, 128 batch size, 150 epochs, 0.05 l2 regularized\n",
    "m6 = models.Sequential()\n",
    "m6.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l2(0.05)))\n",
    "m6.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n",
    "m6.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n",
    "m6.add(layers.Dense(10, activation='softmax'))\n",
    "m6.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m6 = m6.fit(train_x, train_y, epochs=150, batch_size=128, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7: 4 layer, 128 units, 128 batch size, 150 epochs, 0.3 dropout rate\n",
    "m7 = models.Sequential()\n",
    "m7.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
    "m7.add(layers.Dropout(0.3))\n",
    "m7.add(layers.Dense(128, activation='relu'))\n",
    "m7.add(layers.Dropout(0.3))\n",
    "m7.add(layers.Dense(128, activation='relu'))\n",
    "m7.add(layers.Dropout(0.3))\n",
    "m7.add(layers.Dense(10, activation='softmax'))\n",
    "m7.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m7 = m7.fit(train_x, train_y, epochs=150, batch_size=128, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 8: 3 layer, 128 units, 128 batch size, 200 epochs, 0.3 dropout rate\n",
    "m8 = models.Sequential()\n",
    "m8.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
    "m8.add(layers.Dropout(0.3))\n",
    "m8.add(layers.Dense(128, activation='relu'))\n",
    "m8.add(layers.Dropout(0.3))\n",
    "m8.add(layers.Dense(10, activation='softmax'))\n",
    "m8.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m8 = m8.fit(train_x, train_y, epochs=200, batch_size=128, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 9: 3 layer, 512 units, 512 batch size, 50 epochs, 0.005 l1 regularized\n",
    "m9 = models.Sequential()\n",
    "m9.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.005)))\n",
    "m9.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.005)))\n",
    "m9.add(layers.Dense(10, activation='softmax'))\n",
    "m9.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m9 = m9.fit(train_x, train_y, epochs=50, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 10: 6 layer, 128 units, 128 batch size, 50 epochs, 0.005 l2 regularized\n",
    "m10 = models.Sequential()\n",
    "m10.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l2(0.005)))\n",
    "m10.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
    "m10.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
    "m10.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
    "m10.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
    "m10.add(layers.Dense(10, activation='softmax'))\n",
    "m10.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_m10 = m10.fit(train_x, train_y, epochs=50, batch_size=128, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation set loss over the epochs for all 10 alternative models and the previous 4 models\n",
    "m1_val_loss = result_m1.history['val_loss']\n",
    "m2_val_loss = result_m2.history['val_loss']\n",
    "m3_val_loss = result_m3.history['val_loss']\n",
    "m4_val_loss = result_m4.history['val_loss']\n",
    "m5_val_loss = result_m5.history['val_loss']\n",
    "m6_val_loss = result_m6.history['val_loss']\n",
    "m7_val_loss = result_m7.history['val_loss']\n",
    "m8_val_loss = result_m8.history['val_loss']\n",
    "m9_val_loss = result_m9.history['val_loss']\n",
    "m10_val_loss = result_m10.history['val_loss']\n",
    "\n",
    "# as some models only have 50 epoches, we only compare the results for the first 50 epoches for all models\n",
    "new_epochs = np.arange(1, 51)\n",
    "plt.plot(new_epochs, init_val_loss[:50], label='initial validation loss')\n",
    "plt.plot(new_epochs, drop_val_loss[:50], label='dropout validation loss')\n",
    "plt.plot(new_epochs, l1_val_loss[:50], label='l1 validation loss')\n",
    "plt.plot(new_epochs, l2_val_loss[:50], label='l2 validation loss')\n",
    "plt.plot(new_epochs, m1_val_loss[:50], label='m1 validation loss')\n",
    "plt.plot(new_epochs, m2_val_loss[:50], label='m2 validation loss')\n",
    "plt.plot(new_epochs, m3_val_loss[:50], label='m3 validation loss')\n",
    "plt.plot(new_epochs, m4_val_loss[:50], label='m4 validation loss')\n",
    "plt.plot(new_epochs, m5_val_loss[:50], label='m5 validation loss')\n",
    "plt.plot(new_epochs, m6_val_loss[:50], label='m6 validation loss')\n",
    "plt.plot(new_epochs, m7_val_loss[:50], label='m7 validation loss')\n",
    "plt.plot(new_epochs, m8_val_loss[:50], label='m8 validation loss')\n",
    "plt.plot(new_epochs, m9_val_loss[:50], label='m9 validation loss')\n",
    "plt.plot(new_epochs, m10_val_loss[:50], label='m10 validation loss')\n",
    "plt.ylabel('Validation set loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.title('The validation loss for different epochs for 14 models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model from the ones you have estimated so far - this should have the lowest validation loss score at any potential epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Final model\n",
    "\n",
    "# Reestimate that model using all of the training data (no validation set) with the same hyperparameter values\n",
    "result_m1_reestimate = m1.fit(x_train, y_train, epochs=50, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate the test set loss and accuracy\n",
    "m1.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the model generalize?\n",
    "<br>\n",
    "<br>\n",
    "Why do you think this model performed better/worse than your other model configurations? What about the design of the network led to improved performace?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
