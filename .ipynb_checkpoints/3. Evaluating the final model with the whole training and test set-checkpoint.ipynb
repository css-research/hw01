{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "#load data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "#transform image data into 2d-tensor\n",
    "x_train = x_train.reshape((60000, 28*28))\n",
    "x_train = x_train.astype('float32') / 255\n",
    "\n",
    "x_test = x_test.reshape((10000, 28*28))\n",
    "x_test = x_test.astype('float32')/ 255\n",
    "\n",
    "#make y categorical type\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#check shapes\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbcb0e07f98>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "# Model 4 layer 5, units 128, l2 reg weight 0.001\n",
    "model_4 = models.Sequential()\n",
    "model_4.add(layers.Dense(128, activation = 'relu', \n",
    "                         input_shape=(784,),\n",
    "                         kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "model_4.add(layers.Dense(128, activation = 'relu', \n",
    "                         kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "model_4.add(layers.Dense(128, activation = 'relu', \n",
    "                         kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "model_4.add(layers.Dense(128, activation = 'relu', \n",
    "                         kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "model_4.add(layers.Dense(128, activation = 'relu', \n",
    "                         kernel_regularizer = regularizers.l2(0.001)))\n",
    "\n",
    "model_4.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model_4.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model_4.fit(x_train, y_train, \n",
    "                    batch_size = 512, epochs = 200, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 66us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4529171320438385, 0.8777]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The loss and accuracy look pretty decent; the validation accuracy is about 87.8%. the model I think achieve some degree of generalizability.\n",
    "\n",
    "* The followings are some thoughts after experimenting with neural nets.\n",
    "\n",
    "    - I realized that using a simple model might not produce the best result; for example, I learned that using weight regularization can improve the model fit when it is used appropriately (which should be neither too high or low). Of course, the proper way of implementing sorts of additional settings would depend on the data.\n",
    "    \n",
    "    - The optimal sizes of layers and hidden units also seem to depend on the data; I first thought that increasing the number of layers and hidden units would lead to a better model fit but it was not the case for the data I worked with for this assignment. For instance, when I constructed a model which has 20 layers with 128 hidden units and 0.001 L2 regularization weight, the model stopped minimizing the loss function. I guess the optimization process was stuck to a local minimum and it might have been different with another set of data.\n",
    "    \n",
    "    - I think that similar point could be pointed out regarding the epoch size; a larger epoch size does not seem to guarantee a better result; the model can degrade as the number of epoch increases in some cases.\n",
    "    \n",
    "    - In general, I think I learned why people say that we need a sense of art, rather than science when working with the neural nets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
